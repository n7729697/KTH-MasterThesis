\chapter{Introduction}
\textit{In this chapter, the background of soft quadruped robots and reinforce learning approaches to its control is presented. Formulated research questions are listed together with methodologies. In addition, the limitations and delimitation to this thesis are discussed, as well as the ethics and sustainability analysis.}

\section{Background}
Robots are becoming increasingly ubiquitous in our lives, with applications ranging from manufacturing to healthcare\cite{wangCurrentResearchesFuture2018}. Conventional quadruped robots, i.e. rigid quadruped robots, have the potential to work in unstructured, severe and dangerous environments\cite{mengReviewQuadrupedRobots2016}, some specific examples of how quadruped robots could be used include search and rescue missions\cite{hawkesSoftRobotThat2017}, moving items on industrial production lines\cite{gehringANYmalFieldSolving2021} and scientific research\cite{hewingLearningbasedModelPredictive2020}. Soft robots, in particular, have gained attention in recent years due to their ability to deform and adapt to their environment, making them suitable for tasks that require interaction with humans or delicate objects\cite{muralidharanSoftQuadrupedRobot2021}. However, the control to a soft quadruped robot still require further research to establish the internal connection with control science\cite{wangControlStrategiesSoft2022}.

\ac{RL} has shown promise in developing optimal policies for robots by enabling them to learn from experience through trial and error. \ac{RL} is a type of machine learning where an agent learns to take actions in an environment to maximize a reward signal. \ac{RL} algorithms have been used to develop policies for various types of robots, including quadruped robots, with varying degrees of success.

In the previous studies\cite{thorapallimuralidharanContinuumActuatorBased2020} completed by KTH Mechatronics and Embedded Control Systems Unit, tendon-driven soft continuum actuators were implemented on 3D/4D printed structures to operate as the legs of quadruped robots. Specifically, the tendons are used to transmit the force from motors, while the soft material acts as the core and the rigid disc acts as the tendon guide to form the actuator body. Since soft legs are inherently compliant to the terrain, they have had an increased capability of traversing complicated environments. Based on this soft leg, a soft quadrupedal robot prototype was developed, and the gait analysis were conducted to enable the robot to walk\cite{daneliaStructureGaitOptimizationof2021}. Modelling for the actuators and legs are quite important for closed loop control, since the robots will benefit from feedback to the closed loop control when interacting with the terrine. Therefore, the KTH team\cite{muralidharanSoftQuadrupedRobot2021} have already investigated the modeling process of a quadruped robot enabled by four tendon-driven continuum actuators on MATLAB Simulink. Based on the developed soft robot model, an optimal gait controller was developed by a model-free reinforcement learning approach\cite{jiSynthesizingOptimalGait2022} recently.

\section{Problem Statement}
 The gait controller was first trained and evaluated in the simulation platform, and then it is deployed in the physical robot prototype for experimental testing. The developed gait control can be optimum, but the achieved walking speed on the real robot was half of the target speed in the simulation\cite{jiSynthesizingOptimalGait2022}. The current approach also applies Simulink Multibody functions to describe the robot dynamics, which achieves a relatively accurate physical representation but at a very low time efficiency of the simulator. Thus, the learning efficiency of the \ac{RL} algorithm is limited, and it is desired to be improved. The concept of model-based reinforcement learning is considered to generate a functional representation of the robotâ€™s interaction with the external environment, so as to resemble the physical plant model and provide the state update feedback with high accuracy efficiently\cite{rayModelBasedReinforcementLearning2010}. In model-based \ac{RL}, the agent learns a surrogate model of the environment, which can be used to make predictions about the future state of the environment and the possible outcomes of different actions, which allows the agent to plan ahead and make more informed decisions, allowing it to learn more efficiently and solve tasks more effectively\cite{polydorosSurveyModelBasedReinforcement2017}. Compared to the Simulink Multibody functions, this predictive model is expected to improve the simulation efficiency while guaranteeing the model fidelity, which can benefit the following \ac{RL} gait training. 
 
 Furthermore, the Simscape soft robot model was used as the environment model for the \ac{RL} agent\cite{jiSynthesizingOptimalGait2022}, which monitors the locomotion of soft quadruped robots. Model-based \ac{RL} is prior to model-free \ac{RL} in certain scenarios, some literature\cite{polydorosSurveyModelBasedReinforcement2017,wangBenchmarkingModelBasedReinforcement2019} stated that at robotic applications, the model-based \ac{RL} utilizes a model of the environment to make predictions about the outcomes of different actions, which can lead to more efficient learning. The model-free \ac{RL} learns directly from experience without building a model of the environment\cite{arulkumaranDeepReinforcementLearning2017}, and it often require large amounts of data to learn a policy that can achieve optimal performance\cite{calisirModelFreeReinforcementLearning2019}. With a surrogate model, a model-based \ac{RL} agent can simulate different possible future trajectories, allowing it to select actions that maximize rewards\cite{fazeliSeeFeelAct2019}. Model-based \ac{RL} approach can also help reduce the sample complexity and improve the data efficiency of the learning process\cite{atkesonComparisonDirectModelbased1997}. The policy learnt by the surrogate model is different from that by model-free \ac{RL}\cite{fazeliSeeFeelAct2019}, which might make a significant difference to the policy learnt and affect the walking performance of soft quadruped robots, so the evaluation of the controllers will not focus on the algorithms, but on the benefits to the ability of a robot to walk in terms of stability, walking speed and cost-of-transport. 

 However, the input value to the \ac{RL} agent consists of state space and action space, and the state space was defined by available sensor measurements, including robot moving velocity in three directions, rotational angle in three directions and normalized contact force on four feet\cite{jiSynthesizingOptimalGait2022}. The action space was defined by motors on the legs, and three motors on each leg\cite{jiSynthesizingOptimalGait2022}. Therefore, the state-action space of the reaches 22 dimensions, which expands the computational requirements and leads to sparsity in the reward function. As discussed in the background, the state space of the surrogate model consists of state transitions states and reward function states, which will also reach a significant high dimensions. In order to increase the learning efficiency, there could be several ways to restrict the state space of the plant model, one of them could be restricting the state-action space in the model, for example, the feature extraction, which means some relevant features could be extracted and composed based on the kinematics of certain gaits. In the previous training\cite{jiSynthesizingOptimalGait2022}, the gait controllers were learnt from scratch, but rigid quadruped robots can have certain patterns of leg movements, including trot, pace, bound, pronk, gallop, etc.\cite{zhongAnalysisResearchQuadruped2019}, so that if the gait controllers could be defined on certain patterns, the gait controllers could be simplified by some certain gaits and some actions could be composed to reduce the state-action space so as to increase the learning efficiency\cite{owakiQuadrupedRobotExhibiting2017}. The trot is a common gait in all domestic quadrupeds\cite{jiOmnidirectionalWalkingQuadruped2022}. It is well-suited for rough, irregular ground and for traveling long distances at a fair rate of speed, because work is spread evenly over all four limbs, and diagonal support makes it easy to maintain equilibrium\cite{fletcherTrot2012}, so it is considered to restrict the pattern of a soft quadruped robot to trot. Another method to restrict the state space of the plant model is parameterization, the phases between gait generator and control policy could be studied and defined based on the abstraction of the quadruped robot to parameterize various gaits for robust control of rigid quadruped robots\cite{shaoLearningFreeGait2022}. Based on previous parameterization on the continuum actuators\cite{jiOmnidirectionalWalkingQuadruped2022}, such techniques could be considered to be applied on the surrogate model so as to increase the learning efficiency by reducing the search space of the state space.
 \subsection*{Research Questions}
Therefore, the research questions come out as: 
\begin{enumerate}
    \item \label{rq1}How to restrict the state space or design a surrogate model with high estimation accuracy of soft quadruped robots plant compared to using Simulink Multibody functions? This model could be extracted as a representation of the real system, allowing for efficient and accurate simulations and training. Some methods considered to restrict the state space:
    \begin{enumerate}
        \item Feature extraction, it involves selecting a subset of features based on the certain pattern of quadruped robots, i.e. trot.
        \item Parameterization, the higher-level abstractions of the state-action space, it will use phases between gait and real motors to parameterize gaits of quadruped robots.
    \end{enumerate}
    \item \label{rq2}In comparison to model-free \ac{RL}, to what extend can the model-based \ac{RL} approach generate a better \ac{RL} agent and enhance the ability of a soft quadruped robot to walk in terms of stability, walking speed, and cost-of-transport? The enhancement than model-free \ac{RL} is a benchmark of this project. Furthermore, it is important to evaluate the performance of model-based \ac{RL}, and the evaluation of performance of this project focus on the simulation benefices, so what the trade-off is among the learning efficiency, the simulation accuracy and the long-term planning accuracy in order to train an optimal policy for gait control of soft quadruped robot?
\end{enumerate}

\section{Objectives} 
Due to the complexity of the state space of robot models, a limited amount of collected data may not provide a comprehensive understanding of the dynamics of the quadruped robot, which can result in decreased accuracy. The learning efficiency of current approaches to an optimal gait controller is low and expected to be improved. Therefore, this project aims to explore and address these issues by using a model-based reinforcement learning approach for optimal gait control of soft quadruped robots. 

% \section{Methodologies}
% The main methods and methodologies applied towards answering the research questions are discussed in this section. 

% \textbf{Model design}
% \begin{enumerate}
%     \item Extract the signals to motors from the typical trot of quadruped robots by studying the actuation of soft quadruped robot trot pattern.
%     \item Obtain the model of soft quadruped robots and use it to generate simulation data using the Simscape model of the soft quadruped robot based on the extracted state space. 
%     \item On the basis of proceeded data, design a surrogate model with high estimation accuracy of soft quadruped robot plant, which can effectively simulate the dynamics of the system and can be used to train an optimal policy for gait control. 
%     % \item Preprocess the simulation data to reduce the dimensionality of the state space using the methods described in Research Question 1.
% \end{enumerate}
% \textbf{Model-based \ac{RL} algorithm development and comparison}
% \begin{enumerate}
%     \item Develop a model-based \ac{RL} algorithm for gait control of the soft quadruped robot using the extracted features and the reduced state space.
%     \item Evaluate the algorithm's performance in simulation using metrics such as stability, walking speed, and cost-of-transport.
%     \item Iterate on the algorithm's design and parameters to improve its performance.
%     \item Evaluate and compare the previous model-free \ac{RL} algorithm's performance with the same metrics.
% \end{enumerate}
% \textbf{Evaluation and analysis}
% \begin{enumerate}
%     \item Evaluate the trade-offs between learning efficiency, simulation accuracy, and long-term planning accuracy in the context of training an optimal policy for gait control of the soft quadruped robot.
%     \item Analyze the results and draw conclusions about the effectiveness of the proposed model-based \ac{RL} approach compared to model-free \ac{RL}.
% \end{enumerate}
% \textbf{Validation}
% \begin{enumerate}
%     \item Validate the proposed approach by implementing the optimal policy on the physical soft quadruped robot and measuring its performance in a real-world setting, measure the robot's performance in terms of walking speed, stability and cost-of-transport.
%     \item Compare the physical robot's performance with the simulated results to validate the accuracy of the simulation and the effectiveness of the proposed approach.
% \end{enumerate}

\section{Limitations and Delimitation}
\subsection*{Limitations}
blabla

\subsection*{Delimitation}
 This thesis will focus on exploring and addressing the issues of model-based reinforcement learning for optimal gait control of soft quadruped robots. For this particular study, there are several delimitations important to consider. Firstly, it focuses specifically on soft quadruped robots and does not take into account other types of robots or robotic systems. Secondly, simulations and experiments are limited to a controlled environment and may not accurately reflect real-world scenarios. Thirdly, the effects of external factors such as wind, terrain, or obstacles are not considered in this study. Additionally, the project assumes that the soft quadruped robot is functioning correctly and does not address potential hardware malfunctions or failures. Finally, this project is limited to a specific model-based algorithm and does not consider other potential approaches or methods.
 
\section{Ethics and Sustainability}
The use of robotics and artificial intelligence raises ethical and sustainability considerations that need to be addressed in this project. One ethical consideration is related to the potential for the quadruped robot to be used for military or surveillance purposes, which could have negative impacts on privacy and human rights. To ensure that the robot is used ethically, the project will focus on the development of an optimal gait controller for soft quadruped robots for use in research and other non-military applications. Sustainability considerations include the environmental impact of the materials used in the construction of the robot, as well as the potential impact of the project on the environment through energy consumption and waste. To address these considerations, the project will use environmentally-friendly materials where possible and focus on energy efficiency in the design and testing of the robot.

\section{Outline}
This paper is divided into 6 chapters. Chapter 1 introduces the fundamental information of the research topic and the methodology used to study on the research questions. Chapter 2 introduces the methods used to answer the research questions. This chapter deals with the design of soft continuum actuator and the design of the test rig used to test the dynamics of the actuators. It also details the design of the soft quadrupedal robot, including its mechanical and electrical design and the method to generate gaits. Chapter 3 introduces the experiments conducted for the research questions and the results of the experiments. This chapter also includes the structure adjustment of the soft actuators for experiments. Chapter 4 presents the conclusion of the thesis project and the future work.

Boxes can be used to organize content
\begin{tcolorbox}[title={Development environment for prototype}]
	\tt{
		\textbf{Operating systems }\\
		computer: Linux - kernel 4.18.5-arch1-1-ARCH\\
		android phone: 8.1.0\\
		~\\
		\textbf{Build tools}\\
		exp (build tool): version 55.0.4\\
		~\\
		...
	}
\end{tcolorbox}