\chapter{State of the Art}
\label{chap2}
% \thispagestyle{fancy}
\textit{In this chapter, a detailed description about background of the degree project is presented together with related work. This chapter reviews the current technological advances and identifies the research gaps in optimal control of soft quadruped robots and model-based reinforcement learning.}

\section{Related Work}
The process of modeling a robot is a fundamental step in developing an effective controller\cite{gromovModelingControlRobotic2019}. Modeling provides a means to understand the system's dynamics, which is crucial for designing controllers capable of precise control of the robot's locomotion. However, when it comes to soft legs with soft actuators, the modeling and control process becomes more challenging in comparison to rigid legs, which can be modeled using multi-body dynamics for their dynamic movements, This is due to the high non-linearity\cite{slotineAppliedNonlinearControl1991} and time variant properties\cite{wangControlStrategiesSoft2022} of soft materials, where the high non-linearity of soft materials makes them difficult to predict and control accurately, and the hysteresis and drift will introduce more unpredictable behaviors. Furthermore, the need to consider continuum mechanics, which deals with the behavior of continuously deformable materials with infinite \ac{DoF}\cite{polygerinosSoftRoboticsReview2017}, adds an extra level of complexity. Despite these challenges, some research groups from KTH Royal Institute of Technology\cite{jiSynthesizingOptimalGait2022,daneliaStructureGaitOptimizationof2021,thorapallimuralidharanContinuumActuatorBased2020,jiLearningbasedControl4D} have built a soft quadruped robot based on soft continuum actuators, which is capable of walking. Nonetheless, this robot does not move optimally, which highlights the need for further research and development in this area. They also investigated the modelling of soft quadruped robot. 

\subsubsection{motor-driven units}
Soft continuum actuators are a type of soft actuator that can produce a continuous bending or twisting motion along the length of the actuator, without the need for discrete joints or segments. In the previous work\cite{muralidharanSoftQuadrupedRobot2021}, the it is presented the design and fabrication of a quadruped robot enabled by four TSAs as shown in Fig. 1(a). Each actuator is driven by three servo motors to enable omnidirectional bending motions.
\ac{FR}, \ac{FL}, \ac{RR}, \ac{RL2} legs

\subsubsection{tendon transmission mechanism}
\ac{IMU} Quaternion to represent the motion, Quaternions are a mathematical construct that extends the notion of complex numbers to four dimensions. They consist of four components, namely the scalar part, denoted by $w$, and the vector part, denoted by $\vec{v}=(x, y, z)$, and can be expressed as:
$$q = w + xi + yj + zk$$
where $x$, $y$, $z$, and $w$ are real numbers, and $i$, $j$, and $k$ are three imaginary units that satisfy the following multiplication rules: $i^2 = j^2 = k^2 = ijk = -1$. This approach entails defining the initial pose of the robot as a quaternion and obtaining the current pose using sensors such as accelerometers and gyroscopes. The change in pose between the initial and current poses is then calculated, and the rotational component is extracted. From the rotational component, the rotation axis and angle can be determined. The velocity of the robot can be obtained by differentiating the position component of the current pose with respect to time. The velocity vector is then transformed into the world coordinate frame using the current orientation of the robot. The angular velocity vector and the linear velocity vector are the final output of the calculation.
Force sensors 
\ac{ToF} sensor 
Optical camera
\subsubsection{flexible structure deformation system}


\subsubsection{ground contact subsystem}
STM32 

The robot design is a sophisticated design for this project, and only part of sensors will be used to guide the optimal gait controller design. The servo motor used is TS-411MG from TrackStar, which receives \ac{PWM} signals and rotates the shaft proportionally using the built-in position controller.  With all subsystems modeled, the quadruped robot system is assembled and simulated in the MATLAB Simscape environment. In this study, to enable enough bending of the actuator while still maintaining the balance of the entire robot, the maximum rotational angle of the servo motor is set as $\bar{a}_M = \pi/6$, meaning that the rotational range of the servo motor is as [-$\bar{a}_M$,$\bar{a}_M$]. The corresponding \ac{PWM} signal range is [6.67\%, 8.33\%]. The tendon-driven continuum actuator is composed of a sequential combination of rigid disks and flexible cores. The three driven tendons are equally distributed at a radial formation around the rigid disks of the actuator and passed through respective guiding holes placed on the rigid disks. Furthermore, the length of the actuators with core and disk structure is also reduced and the upper thigh is printed as solid to offer more rigidity. To model the above soft actuator deformations, the following two methods can be applied: Use lumped parameter method, where rigid components are applied to build the shape of the cores and the disks while using spring and damper joints for modeling the connections between the cores and disks and enabling the target bending and compression actuator motions. These joints use equivalent stiffness and damping to provide equal response of the entire actuator compared with the flexible segments, only needs to estimate the rigid connecting joint motions with stiffness $k$ and damping $d$ properties in the bending and longitude directions, denoted as $k_B$, $d_B$, $k_L$ and $d_L$ respectively. The tendons pass through the guide holes in the rigid disks, which allow tendon motion while holding them in place. The tendons can be simulated using the cable functions in most simulations tools, where the continuum actuators are then created in the MATLAB Simscape Multibody environment by the rigid components with telescope joints and no pulley. The contact between the feet of the quadruped robot and the ground is represented using the static and dynamic friction coefficients $f_s=0.315$ and $f_d=0.3$. The local coordinate system of the robot is defined using the right hand rule, with the front direction as the $x$ direction and the anti-gravity direction as the $z$ direction. Ten key parameters are drawn from the simulation model to represent the robot states. The roll, pitch and yaw movements are represented using the rotational angles of the body center in the three axes $x$, $y$ and $z$, noted as $\theta_x$, $\theta_y$ and $\theta_z$. The translational moving velocities are also represented by the movement of the body center in the three directions and are noted as $v_x$, $v_y$ and $v_z$, respectively. The update period of the RL algorithm is $T_s$ = 0.05s and the simulation model of the soft robot is a continuous-time model. To prevent the robot from falling down or entering unstable states in which the simulation results would be unreliable, termination boundaries are set in the simulation environment. When the robot states meet these termination conditions, the simulation ends before the desired simulation time. The roll and pitch angles of the body center $\theta_x$ and $\theta_y$, are set to rotate less than 0.15 rad, while the bending angle $alph_b$ of all the four legs are set to less than $\pi/3$.

The servo motor that receives \ac{PWM} signals and generates rotational torque via its shaft, which is then transmitted to the following subsystems. It is assumed that the rotational angle of the servo motor follows the input \ac{PWM} signals without error. The spool that is connected to the shaft of the servo motor converts the motor torque to the force in the tendon. The two ends of the tendon are connected to the spool and the end disk of the following continuum structure. When the spool rotates, the rotational motion of the spool also causes a corresponding length change of the tendon. The middle position 0 of the spool is considered as the neutral position, where the actuator is in its straight shape. When the spool angle rotates to positive values, the tendon is pulled up. For negative values, the tendon turns into slack mode. The tendon force is applied on the actuator and causes deformation. The force balance between the tendon force and reaction force of the actuator determines the deformation dynamics. Three of the motor and spool systems apply forces at equally distributed radial locations of the actuator and can lead to omnidirectional bending motion. The morphing of the actuators is realized by the traction force exerted via the tendons. Four of the above actuator systems are assembled together as the four legs of the quadruped robot. The robot is placed on the ground with gravitational forces in the simulation environment. The different motions of the four legs cause relative motion with the ground and generate static or dynamic friction, which then lead to the robotâ€™s movement. Different ground conditions have different frictional properties and can result in different motions.


\subsection{Modeling in MathWorks Simulink\texorpdfstring{\textsuperscript{\textregistered}}{(R)}}
The model of this quadruped robot comprises several subsystems, including the motor-driven units, the tendon transmission mechanism, the flexible structure deformation system, and the ground contact subsystem. 

In addition, because of the influence of external gravity, the force required to move the motor at the same angle is different for different leg suspension positions, so the leg model of the quadruped is nonlinear, so that it is common to use a model predictive controller for rigid quadruped robots. However, it is not easy to predict the behavior of a soft robot if using a model predictive controller\cite{BemporadLinearTimevaryingNonlinearMPC}, so it is desired to have more identical solutions for the controllers, whereby the  reinforcement learning came to our mind\cite{hewingLearningbasedModelPredictive2020}. Typically, the loads of rigid legs are different between the two cases of leg suspension and stepping on the ground\cite{biswalDevelopmentQuadrupedWalking2021}, which means that the dynamics of the quadruped robot is a mixture of continuous and discrete, such that the model is continuous when touching the ground or when hovering, but there is a sudden change from hovering to touching the ground, which makes the model behave a discrete. 

Additionally, applying \ac{RL} to train the gait controller involves creating a virtual model of the robot and its environment to simulate possible walking behaviors and test the efficacy of various control policies. This approach is motivated by the challenges associated with collecting data in the physical world, which can be time-consuming, costly, and hazardous. By leveraging the benefits of simulation, the training process can generate a large volume of data quickly and inexpensively, allowing for the exploration of a broad range of behaviors and control policies without exposing the physical robot to potential risks. This methodology facilitates the application of deep learning and reinforcement learning techniques to develop a robust and efficient gait controller for the soft quadruped robot. The simulation-based approach allows for efficient iteration and optimization of the training process, resulting in a control policy that can be effectively applied to the physical robot.
\section{Reinforcement Learning}
As the complexity of robotics increases, \ac{RL} is increasingly required to develop robust control algorithms with high \ac{DoF}s and time variant properties\cite{zhangEffectiveSoftRobot2017}. The rapid development of AI provides an alternative solution to take into account the nonlinear properties of novel materials and soft robots\cite{tangModelbasedOnlineLearning2021}. \ac{MFRL} is a type of reinforcement learning where an agent learns to make decisions based on experiences and rewards obtained from the environment, without having an explicit model of the environment. The agent directly can map observations to actions, through trial and error, without considering the underlying dynamics of the environment\cite{arulkumaranDeepReinforcementLearning2017}. The goal of model-free reinforcement learning is to learn a policy $\Pi$, which is a mapping from states to actions, that maximizes the expected cumulative reward over time. The \ac{MBRL} refers to a type of \ac{RL} where an agent learns optimal behavior by learning a model of the environment by taking actions and observing the outcomes that include the next step and the immediate reward\cite{rayModelBasedReinforcementLearning2010}. 
\input{figures/demo}
Typically, the model can be considered as a combination of state transition distribution $P_\eta$ and reward function $R_\eta$, $$M = (P,R) \textrm{ where } s_{t+1}\sim P_\eta(s_{t+1}|s_t, a_t) \textrm{ and } r_{t+1}\sim R_\eta(r_{t+1}|s_t, a_t)$$ $s_t$ is the state at time $t$, $r_t$ is the reward at time $t$ and $a_t$ is the action at time $t$. \ac{MBRL} learns an abstract model of the environment to plan the optimal policy. This model can be used to simulate the consequences of its actions, allowing it to make informed decisions, which makes it applicable to handle more complex environments, as it can make use of a model of the environment's dynamics. Therefore, model-based \ac{RL} requires a prior knowledge of the domainology\cite{tangModelbasedOnlineLearning2021}, as a model needs to be created and maintained. The domain of the model-based \ac{RL} refers to the context or environment in which the \ac{RL} agent is operating, which determines the available actions, observations, and rewards, and hence influences the behavior of the agent\cite{langExplorationRelationalDomains}. In model-based \ac{RL}, the agent needs to build a model of the dynamics of the system within the domain, and this model should accurately represent the true behavior of the system in order for the agent to learn effectively.
\subsection{Data-Driven Method}
A data-driven approach for training the gait controller of a soft quadruped robot by \ac{MBRL} relies on the collection of empirical data to inform the control policy. However, this method presents various challenges, such as the need for specialized equipment and controlled experimental conditions, and may not be feasible in certain scenarios. Alternatively, a \ac{MBRL} approach employs a virtual model of the robot and its environment to generate training data in a rapid and cost-effective manner, thus enabling the exploration of diverse control policies and behaviors. This methodology offers advantages in terms of scalability and efficiency compared to a purely data-driven approach and is well-suited for training the gait controller of a soft quadruped robot. 

\subsection{Model-based Reinforcement Learning}

\section{Surrogate Model}

\subsection{Neural Networks}
Here are some pros and cons of three commonly used optimization algorithms:

\ac{SGD}:
Pros:
It is a simple and computationally efficient optimization algorithm.
It works well for large datasets and can handle noisy or sparse gradients.
It can be used for both shallow and deep neural networks.
Cons:

It may get stuck in local minima and can be slow to converge.
It requires careful tuning of the learning rate.
Adaptive moment estimation (Adam):
Pros:
It is a fast and computationally efficient optimization algorithm.
It uses adaptive learning rates for each parameter and can handle sparse gradients.
It can converge quickly and has been shown to work well for a wide range of deep learning problems.
Cons:

It can converge to suboptimal solutions in some cases.
It may require more memory compared to \ac{SGD}.
Root mean square propagation (RMSprop):
Pros:
It adapts the learning rate based on the average of the recent squared gradients and can handle sparse gradients.
It can converge quickly and has been shown to work well for deep neural networks.
Cons:

It may get stuck in local minima in some cases.
It may require careful tuning of the learning rate and other hyperparameters.
It's important to note that these are just some general pros and cons of each optimization algorithm, and the choice ultimately depends on the specific problem and the network architecture. In practice, it's common to try multiple optimization algorithms and choose the one that works best for the specific problem.

The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.

While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model's posterior probability can be used as an inverse cost).

Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as Extreme Learning Machines, "No-prop" networks, training without backtracking, "weightless" networks, and non-connectionist neural networks

Stochastic gradient descent (SGD) is an iterative optimization algorithm commonly used in machine learning to train neural networks. It is a variant of gradient descent that updates the model parameters (weights and biases) on a per-sample basis, rather than accumulating the gradients over the entire training set.

In SGD, the training data is typically divided into small batches (mini-batches), and the model parameters are updated after processing each batch. The algorithm computes the gradient of the loss function with respect to the parameters using the current mini-batch, and then updates the parameters by taking a step in the direction of the negative gradient.

The advantage of SGD is that it can converge faster than batch gradient descent, especially when the training set is large. It also has lower memory requirements, since only a small batch of training data needs to be loaded into memory at a time.

However, the updates in SGD can be noisy due to the small batch size, which can lead to instability and oscillations in the training process. To mitigate this issue, several variants of SGD have been proposed, such as momentum-based methods, adaptive learning rate methods (e.g., Adam, Adagrad), and variants that combine multiple mini-batches (e.g., mini-batch gradient descent).
Here are some general pros and cons of some commonly used optimization algorithms in deep learning:

Stochastic Gradient Descent (SGD):
Pros:
Simple and easy to implement.
Fast convergence on simple problems.
Cons:

Can converge slowly on problems with more complex and non-convex loss surfaces.
Can get stuck in local minima.
Adaptive Moment Estimation (Adam):
Pros:
Efficient and performs well on a wide range of problems.
Adapts to the shape of the loss surface and can handle noisy gradients.
Cons:

Can converge to suboptimal solutions.
May require some tuning of the learning rate and momentum hyperparameters.
Nesterov Accelerated Gradient (NAG):
Pros:
Can converge faster than traditional SGD.
Resilient to high condition number and high curvature problems.
Cons:

Slightly more complex than traditional SGD.
Can be sensitive to the choice of momentum hyperparameter.
Root Mean Square Propagation (RMSprop):
Pros:
Efficient and performs well on a wide range of problems.
Adapts to the shape of the loss surface and can handle noisy gradients.
Cons:

Can converge to suboptimal solutions.
May require some tuning of the learning rate and momentum hyperparameters.
Adaptive Gradient Algorithm (AdaGrad):
Pros:
Efficient and performs well on sparse datasets.
Adapts to the shape of the loss surface and can handle noisy gradients.
Cons:

May converge too quickly on some problems and not converge at all on others.
May require some tuning of the learning rate hyperparameter.
Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS):
Pros:
Converges quickly and efficiently on smooth, non-convex optimization problems.
Doesn't require the learning rate hyperparameter.
Cons:

Can be computationally expensive and memory-intensive for large datasets.
Can get stuck in local minima.

\section{Theory}

One approach to RL is model-based RL, where the agent learns a model of the environment dynamics and uses this model to plan its actions. Model-based RL has the potential to improve sample efficiency, which is crucial for robotics applications where data collection can be time-consuming and expensive. Model-based RL has been successfully applied to various robotics tasks, including manipulation and locomotion. However, the effectiveness of model-based RL depends on the accuracy of the learned model, which can be challenging to achieve in complex environments.

Several studies have investigated the use of model-based RL for quadruped robots. For example, Ha and Schmidhuber (2018) proposed an approach called world model-enhanced RL, where the agent learns a compact world model of the environment dynamics and uses it to plan its actions. They applied this approach to a simulated quadruped robot and achieved superior performance compared to other RL methods. Similarly, Chowdhary et al. (2020) proposed a model-based RL approach for quadruped robots that used a Gaussian process regression model to estimate the dynamics of the robot's locomotion. They demonstrated the effectiveness of their approach on a physical quadruped robot.

Soft robots have also been the subject of RL research. For example, Geijtenbeek et al. (2018) developed an RL-based controller for a soft robot arm that could grasp and manipulate objects. They used a model-free RL algorithm called deep deterministic policy gradient and demonstrated that the controller could adapt to different object shapes and sizes. Meanwhile, Yao et al. (2020) proposed a model-based RL approach for controlling a soft robotic arm with a cable-driven actuation system. They used a physics-based model of the arm and achieved superior performance compared to other RL methods.

Overall, the existing literature suggests that model-based RL has the potential to develop optimal policies for quadruped robots, including soft quadruped robots. However, the effectiveness of the approach depends on the accuracy of the learned model and the complexity of the environment. In addition, most existing studies have focused on simulated environments, and there is a need for further research on applying model-based RL to physical soft quadruped robots in real-world settings.
Each optimization algorithm supported by trainNetwork has its own advantages and disadvantages, and the choice depends on the specific problem and the network architecture. 

