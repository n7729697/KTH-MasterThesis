\chapter{Results}
\label{chap5}
\textit{This chapter presents the results of the experiments and evaluations conducted in this research. It provides a detailed analysis of the performance of MBRL algorithms with continue training and provide insights into the research questions posed at the beginning of this study.}

\section{Surrogate Model Performance}
\subsection{Performance Evaluation}
To evaluate the surrogate model's performance, it is important to consider several critical metrics. These metrics serve as crucial indicators of how effectively the model approximates and forecasts the robot's behavior. Here, these metrics are elaborated without delving into details:
\begin{itemize}
    \item Average Steps before Failure: This metric assesses the model's capacity to predict the duration of a simulation run without encountering failure, serving as an indicator of sample efficiency.
    \item Validation RMSE and Validation Loss: This RMSE quantifies the disparity between the surrogate model's predictions and the actual values present in the validation dataset, as defined in Equation \ref{eq:RMSE}. Lower RMSE values are indicative of a higher degree of precision and accuracy in forecasting future states ($s_{t+1}$). Concurrently, the Validation Loss serves as an overarching measure of the model's performance on the validation dataset, underscoring the significance of minimizing this metric to enhance predictive capabilities, as defined in Equation \ref{eq:loss}.
    \item NRMSE: NRMSE provides a normalized measure of error, enabling comparisons of prediction accuracy across expert pattern datasets. It proves particularly valuable in evaluating the surrogate model's ability to predict sensor observations on the robot. The single step prediction NRMSE is calculated by Equation \ref{eq:NRMSE}, the long-term prediction NRMSE is calculated by Equation \ref{eq:NRMSET}.
    \item Correlation Coefficient (R): The R quantifies the linear relationship between simulated and predicted values, as specified in Equation \ref{eq:R}. A higher R signifies a more robust correlation, highlighting the model's proficiency in capturing underlying data trends and patterns.
\end{itemize}

\subsection{Control Variables Restriction}
The investigation commences with a focused exploration of state-space restriction within the surrogate model. The state-space, encompassing both observations and actions, is defined as $\mathbf{s}_t = [\mathbf{\theta}(t), \mathbf{v}(t), \mathbf{f}n(t), \mathbf{a}_{t-1}]$ in Section \ref{sec:ss}. It is worth noting that the first three terms in this state-space originate from observations and are inherently challenging to restrict, but the action space can be subject to constraints.  It's crucial to acknowledge that the first three terms in this state-space comes from observations and present inherent challenges for restriction, but the action space can indeed be subjected to constraints. 

An important aspect to underscore is that the restriction of the robot's action space was achieved through the application of pattern-defined modeling techniques. Specifically, the robot's legs are interconnected in diagonal pairs, similar to a trotting pattern. This inherent structural arrangement significantly simplifies the action space compared to dealing with the complexity of controlling 12 individual motor actions\cite{jiSynthesizingOptimalGait2022}, effectively reducing it from $\mathbf{a}\in\mathbb{R}^{12}$ to $\mathbf{a}\in\mathbb{R}^4$. 

Furthermore, an alternate method of parameterization was applied, where the surrogate model's configuration incorporates specific control variables. Within this context, the actions are as defined in Section \ref{Sec:as}, taking the form of $\mathbf{a}_{t} = [\alpha_{b_1}, z_{l_1},\alpha_{b_2},z_{l_2}]$. These variables are subject to the bounds governed by the gains ($\alpha_{b_{gain}}\, , z_{l_{gain}}$). It's also crucial to emphasize that these gains associated with actions hold significance not only in the construction of the surrogate model but also play a vital role in shaping the behavior and adaptability of the SoftQ. Therefore, our focus centers on two key components within the action space: the gain of bending angle $\alpha_{b_{gain}}$ and the gain of compression length $z_{l_{gain}}$ for each of the diagonal leg pairs. To address this aspect, a series of simulations involving random actions within a bounded action space were carried out. These simulations aimed to gather the necessary data required for training the surrogate model. Subsequently, the performance of the trained model was evaluated. To effectively assess the trained neural networks performance, a color heat map can be employed to provide a visual representation of various performance metrics, as shown in Figure \ref{fig:NN_heat}. Each metric is associated with a specific color scale, allowing for a quick and intuitive evaluation.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{img/chap5/NN_heat.eps}
    \caption{Visualization of Performance Metrics Through Color Heat Map for Evaluating Trained Surrogate Models. The metrics include: (a) Average Steps before Failure; (b) \ac{RMSE} on the Validation Dataset; (c) Loss on the Validation Dataset; (d) Single Step Prediction Coefficient R; (e) Average Coefficient R for Long-term Prediction; (f) Coefficient R for Full Horizon Prediction; (g) NRMSE for Single Step Prediction; (h) Average NRMSE for Long-term Prediction; (i) NRMSE for Full Horizon Prediction.}
    \label{fig:NN_heat}
\end{figure}

In Figure \ref{fig:NN_heat}(a), it is evident that reducing the lower restrictions within the action space leads to an increase in the average number of steps the system can take before encountering failure, indicating improved data efficiency due to reduced locomotion risk with smaller actions. In Figure \ref{fig:NN_heat}(c), all neural networks converge with a loss below 0.037 in the validation dataset, with minimal differences between them. In Figure \ref{fig:NN_heat}(b), validated in the same dataset, RMSE converges to a minimum level. Notably, better validation RMSE is achieved when $\alpha_b$ ranges from 0.55 to 0.7 radians and $z_l$ ranges from 3 to 9 millimeters, suggesting optimal model performance within these parameter ranges.

In assessing single-step predictions using various expert gait pattern datasets, the correlation coefficient (R) between predictions and ground truth was calculated, as depicted in Figure \ref{fig:NN_heat}(d). The results show consistently high accuracy when either the compression length ($z_l$) or the bending angle ($\alpha_b$) is high, or when both are elevated. This observation aligns with the expectation that for the robot to achieve higher speeds, it should exhibit more pronounced behaviors, which are characterized by larger values of these parameters. Consequently, the expert gait dataset used in this context emphasizes high values of $\alpha_b$ and $z_l$, resulting in smaller differences between predictions and ground truth, and consequently, lower values of R. A similar trend is observed when considering single-step predictions. Smaller NRMSE values are associated with high values of $z_l$ and $\alpha_b$, but interestingly, low $\alpha_b$ values also yield small NRMSE values. This implys that bending angle ($\alpha_b$) exerts a more significant influence on robot locomotion. These results are visually represented in Figure \ref{fig:NN_heat}(g). However, it is important to note that neither the NRMSE nor R reach their lowest points in the highest $\alpha_b$ region, suggesting poorer predictions when too small steps are taken before simulation failure, resulting in an insufficient amount of useful data.

For multi-step predictions on the same expert gait pattern datasets, the effects of $\alpha_b$ and $z_l$ are similar in single step prediction, but the models with low $\alpha_b$ values perform less effectively in predicting over longer time horizons, suggesting a lack of robust generality in these models. However, as shown in Figure \ref{fig:NN_heat}(e), the average correlation coefficient (R) for long-term predictions reveals that models with relatively high values of both $\alpha_b$ and $z_l$ struggle to predict states over extended horizons. This challenge could be attributed to the larger action space, which introduces more unpredictability and subsequently leads to reduced performance compared to models with lower $z_l$ values when trained on the same dataset size. This trend is also evident in the full horizon prediction R, as depicted in Figure \ref{fig:NN_heat}(f), where predictions at relatively high values of both $\alpha_b$ and $z_l$ deteriorate. In addition, the average NRMSE of long-term predictions exhibits a similar performance to what was observed in single-step predictions. Lower errors are associated with larger values of $\alpha_b$ and $z_l$, but NRMSE also decreases when $\alpha_b$ reaches its highest values. These results are presented in Figure \ref{fig:NN_heat}(h). Interestingly, performance remains consistent when $\alpha_b$ is smaller than 0.72 radians and $z_l$ is larger than 3 millimeters. Likewise, in the case of NRMSE for full horizon prediction, there is no significant change in performance, as indicated in Figure \ref{fig:NN_heat}(i).

Hence, the optimal action state restrictions can be identified as approximately $\alpha_b$ around 0.6 rad and $z_l$ around 7 mm. However, it's important to consider that these restrictions also impact MBRL training, as they limit the range of exploration within the robot's action space. Therefore, it's preferable to keep the action space as broad as possible. Another viable set of restrictions to consider is $\alpha_b$ around 0.63 rad and $z_l$ around 8 mm for the state space during surrogate model training and MBRL training. This broader range allows for more exploration and potentially better overall performance in subsequent MBRL learning.

\section{Model-based RL Training}
After the surrogate model determined in Section \ref{sec:NN_design}, we can continue to training of MBRL for optimal gait controls, which will be used as a gait controller in robot to navigate with learned optimal gait. It's important to note that the training settings remained consistent across all experiments. The code snippet for training details can be found in Section \ref{code:mbrl}. To account for potential variations due to randomness, the training process was initiated with various random seeds. It was observed that training had a higher failure rate when the agent became trapped in local minima. Consequently, it can be intuitively inferred that mastering the policy for low-speed locomotion posed a more challenging task for the agent. Notably, in our experiments, we maintained $v_{ref}$ at no smaller than 0.1 m/s to ensure meaningful training outcomes and the final time of the training in one episode $T_f$ is 10 seconds.
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{img/chap5/MBRL_tr.eps}
    \caption{The Training Results of MBRL. (a) Cumulative reward in MBRL vs the training episodes; (b) The variation of entropy and temperature coefficients during the training process.}
    \label{fig:MBRL_tr}
\end{figure}

The training progress, along with the Entropy and temperature coefficients, is visually represented in Figure \ref{fig:MBRL_tr}. In this plot, you can observe that the Soft Actor-Critic (SAC) algorithm was successful in training the agent. The reward steadily increased from around -45 to 150, signifying the learning of the desired behavior. Additionally, the entropy dropped to around -4 (the goal being the negative of the number of actions), and the temperature also decreased from 1 to 0. These trends indicate that the agent successfully learned the desired behavior during the training process. Following the training process, the determined agent was implemented in simulation to verify its performance. The results of this implementation are presented in Figure \ref{fig:MBRL_val}, providing valuable insights into how well the trained MBRL model performs in executing optimal gaits for the soft robot. After the agent finished trained in MBRL, it was validated in the simulation to have a overview of this agent. As shown in the figure, the robot have some velocity in x axis, but it barely moves and it just 
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{img/chap5/MBRL_val.eps}
    \caption{The Validation Results of MBRL. (a) Cumulative reward in MBRL vs the training episodes; (b) The variation of entropy and temperature coefficients during the training process.}
    \label{fig:MBRL_val}
\end{figure}

To assess the operational capacity and explore the gait patterns of the soft robot, the same RL configurations were applied to learn different walking speeds denoted as $v_{ref}$. Throughout the training experiments, an interesting observation emerged: the training performance of the simulated robot was notably influenced by the reference speed $v_{ref}$. When the target velocity was set higher, the learning agent was able to achieve forward motion in fewer training episodes, ultimately converging to an effective policy. Conversely, when $v_{ref}$ was set to a lower value, the agent exhibited a tendency to pitch forward, resulting in episode termination. The training process was initiated with various random seeds, and it was observed that training had a higher failure rate when the agent became trapped in local minima. Consequently, it can be intuitively inferred that mastering the policy for low-speed locomotion posed a more challenging task for the agent.

\subsection{Performance Evaluation}
In the performance evaluation of the trained agent, it is important to consider several critical metrics. These metrics serve as crucial indicators of proficiency of the agent, referred to as the controller. Here, these metrics are expounded again avoiding excessive details:
\begin{itemize}
    \item Stability (rad/s): This metric pertains to the maximum angular velocity exhibited by the agent along the z-axis. It provides valuable insights into the agent's ability to maintain stability during operation, a fundamental aspect of its performance. 
    \item Resultant walking speed (m/s): This metric involves assessing the agent's walking speed, particularly by calculating the average speed achieved during validation. It offers a straightforward measure of the agent's mobility and effectiveness in movement.
    \item Cost-of-transport (J/kg/m):  The cost-of-transport metric evaluates the energy efficiency of the agent. It quantifies the amount of energy expended by the agent, normalized by its weight and the distance traveled. This metric is crucial for understanding the agent's energy consumption patterns.
    \item Learning efficiency:  Learning efficiency gauges how quickly the agent converges to an optimal or satisfactory performance level. It can be quantified by measuring the number of iterations or episodes required for the agent to reach convergence. This metric provides insights into the agent's adaptability and learning capabilities.
\end{itemize}

\subsection{Continue Learning}
To accelerate the training process, a predefined expert behavior, denoted as $A^e = [\textbf{a}_{T_0}^e, ..., \textbf{a}_T^e]$, is established. This predefined behavior offers the robot an initial stable gait. Specifically, during the initial second of each training episode, the RL agent emulates human knowledge by executing the predefined behavior $A^e$. Data generated from this process is collected and stored in a replay buffer for subsequent learning phases. This approach aligns with the principles of imitation learning, where the RL agent imitates pre-established behaviors as a form of guidance. While it's important to note that this predefined state-action trajectory doesn't guarantee optimality, it serves as a valuable reference for the agent. Emulating these guided elementary movements enables the agent to swiftly implement and adapt its behavior in real-time scenarios, contributing to efficient online implementation.


Long-term planning, Type: Continuous

\section{Model-based RL Comparison}
\label{Sec:MBRL}

In order to examine the operational capacity and investigate the gait patterns of the soft robot, we applied the above RL setups to learn the walking motions with different speed references $v_{ref}$, where the training settings are identical in each experiment. During the training experiments, we observe that the training performance of the simulated robot is susceptible to the reference speed $v_{ref}$. With a higher target velocity, the agent is able to walk forward in fewer training episodes with a converged policy. On the other hand, with a smaller $v_{ref}$ as the target, the agent is prone to pitch forward and the episode is terminated. We start the training process with various random seeds, and the training has a higher failure rate with the agent stuck in the local minimal. Hence intuitively, we regard mastering the low-speed locomotion policy as a more difficult task for the agent.
\subsection{Compare with MFRL}
So the chosen speed references are 0.2, 0.3, 0.5 m/s. ANCOVA analysis you conducted to compare Model-Based Reinforcement Learning (MBRL) and Model-Free Reinforcement Learning (MFRL) while controlling for the covariate desired walking speed ($v_{ref}$) with respect to the dependent variables, stability, resultant walking speed, "\ac{COT}, learning efficiency, and Long-term planning, pre-defined in Section \ref{Sec:test_def}. 
\subsubsection*{Stability}
The ANCOVA analysis revealed a significant difference in the stability of walking between MBRL and MFRL while controlling for the covariate $v_{ref}$, F(1, N) = [Your F-statistic], p = [Your p-value]. After accounting for the influence of $v_{ref}$, it was found that MBRL demonstrated a significantly higher stability in terms of less velocity in the y-direction and less maximum rotation angle. This suggests that the choice of RL method has a significant positive impact on the stability of walking.
\subsubsection*{Resultant walking speed}
The ANCOVA analysis indicated a significant difference in the resultant walking speed between MBRL and MFRL while controlling for the covariate $v_{ref}$, F(1, N) = [Your F-statistic], p = [Your p-value]. MBRL achieved a significantly higher resultant walking speed. This implies that the choice of RL method significantly affects the resultant walking speed.
\subsubsection{Cost-of-Transport}
An analysis of covariance (ANCOVA) was conducted to examine the impact of RL method on cost-of-transport while controlling for the covariate $v_{ref}$. The results revealed a significant difference between MBRL and MFRL, F(1, N) = [Your F-statistic], p = [Your p-value], even after adjusting for the effects of designed walking speed. Specifically, MBRL exhibited significantly higher \ac{COT}. This suggests that the choice of RL method has a significant positive influence on the energy efficiency of walking.
\subsubsection{Learning efficiency}
The ANCOVA analysis for learning efficiency, with $v_{ref}$ as a covariate, yielded significant results, F(1, N) = [Your F-statistic], p = [Your p-value]. When controlling for $v_{ref}$, MBRL demonstrated significantly faster learning efficiency, as measured by the number of iterations required to converge. This indicates that the choice of RL method significantly affects the efficiency of learning in the context of MBRL for optimal gait control of soft quadruped robot.
\subsubsection*{Long-term planning}
The ANCOVA analysis conducted for long-term planning, while controlling for $v_{ref}$, provided noteworthy results, F(1, N) = [Your F-statistic], p = [Your p-value]. After adjusting for the effects of the covariate $v_{ref}$, MBRL demonstrated significantly greater long-term planning abilities, as evidenced by cumulative reward. This suggests that the choice of RL method significantly impacts long-term planning in robustness of optimal gait control of soft quadruped robot. 

\input{tables/rq2test}

\subsection{Compare with Expert Gait}



\section{Field Test}
The culmination of the MBRL and continue training efforts is the deployment of the trained controller onto the physical SoftQ robot. This section presents the results of the field tests, providing insights into the controller's transferability, generalization capabilities, and its performance in real-world scenarios.

The primary objective of the field tests is to assess the extent to which the controller's learned motor skills can be transferred from the simulated environment to the physical robot. The results demonstrate the remarkable transferability and generalization capabilities of the trained controller. Despite the disparities between the simulation and the real world, the controller seamlessly applies the acquired behaviors in tangible, real-world settings. 

One of the key performance metrics evaluated during the field tests is the robot's locomotion. The trained controller showcases impressive locomotion performance across a range of terrains and conditions. It effectively adapts its gait and motor control to navigate flat surfaces, uneven terrain, slopes, and surfaces with varying friction properties. The controller's ability to maintain stability and achieve locomotion is a testament to its adaptability and robustness.

The outstanding performance observed in the field tests holds significant implications for the power of this methodology enhanced by MBRL.