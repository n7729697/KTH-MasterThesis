\chapter{Model-based Reinforcement Learning}
\label{chap4}
\textit{This chapter comprehensively examines the application of data-driven methods to implement \ac{MBRL} for optimal gait control in soft quadruped robots, while also presenting the robot's control architecture.}

\section{Method of Modelling in MBRL}
\subsection{Notation}
The goal of reinforcement learning is to learn a policy that maximizes the cumulative rewards obtained over a sequence of time steps. At each discrete time step denoted by $t$, an agent interacts with its environment. The agent finds itself in a specific state $s_t$ belonging to the state space $S$, then performs an action $a_t$ from the set of possible actions $A$. Following the action, the agent receives an associated reward $r_t$, which depends on the chosen state and action. Afterward, the agent transitions to a new state $s_{t+\Delta t}$, where $\Delta t$ is the time interval between consecutive time steps. These transitions are determined by the underlying, yet unknown, dynamics function $f$, defined as a mapping from pairs of states and actions to subsequent states $f: (s_t, a_t) \rightarrow s_{t+\Delta t}$. 

In the context of \ac{MBRL}, a surrogate model is developed to approximate the dynamics of the environment. This model is employed to predict the future outcomes of actions and states, aiding the agent in making informed decisions. The learned dynamics function is denoted as $\hat{f}_\theta(s_t, a_t)$, and it is parameterized by $\theta$. This learned function accepts the current state $s_t$ and the action $a_t$ as inputs and produces an estimate of the future state $s_{t+\Delta t}$ that the agent will encounter after taking the specified action. The parameter $\theta$ captures the characteristics of the learned function, determining its behavior. In this thesis, the chosen approach involves representing the learned dynamics function $\hat{f}_\theta(s_t, a_t)$ using a deep neural network. This neural network is designed to capture intricate relationships between the input state and action, allowing it to approximate the complex and often nonlinear dynamics of the environment. 

Before initiating the training process, a dataset $D$ was assembled through the collection of training samples. These samples were generated by initializing the system with various starting configurations denoted as $s_0$. Subsequently, random actions $a_t$ were executed at each time step. These actions were drawn from a probability distribution $p(A)$. The outcomes of these actions were recorded, resulting in a sequence of states $(s_0, ..., s_{t-1}, s_t, s_{t+1}, ...)$. In order to ensure uniform treatment of distinct state components, including orientations, velocities, and forces, a preprocessing step is initiated. This involves the subtraction of the mean from the amassed data, succeeded by division by the standard deviation of the dataset, ultimately contributing to the normalization of the data distribution.

\subsection{Validation}
The surrogate model $\hat{f}_\theta(s_t, a_t)$ was subsequently subjected to a training process employing the dataset $D$. The goal of training was to minimize a specific loss function, specifically half of the \ac{MSE}. The formulation of the loss function is: 
\begin{equation}
    Loss = \frac{1}{|D|}\sum_{(s_t,a_t, s_{t+1}) \in D}^{D} \frac{1}{2}\lVert s_{t+1}-\hat{f}_\theta(s_t, a_t)\rVert^2
\label{eq:loss}
\end{equation}
where $\lVert\cdot\rVert$ denotes the Euclidean norm. Moreover, the \ac{RMSE} was employed to quantify the accuracy of the predictions made by the surrogate model. This metric provides a comprehensive assessment of the residual errors between the predictions and the true values. The \ac{RMSE} is computed as: 
\begin{equation}
    RMSE = \sqrt{\frac{\sum_{D} \frac{1}{2}\lVert s_{t+1}-\hat{f}_\theta(s_t, a_t)\rVert^2}{|D|}}
    \label{eq:RMSE}
\end{equation}
During the training process utilizing the dataset $D$, both the \ac{MSE} and \ac{RMSE} in the aforementioned expressions were computed not only on the training dataset but also on a distinct validation dataset $D_{val}$, which was not part of the training dataset. Additionally, the surrogate model's performance was evaluated on a real-world test gait dataset, denoted as $D_{real}$, employing the \ac{NRMSE}:
\begin{equation}
    NRMSE = \sqrt{\frac{1}{D_{val}}\sum_{D_{val}}\lVert \frac{s_{t+1}-\hat{f}_\theta(s_t, a_t)}{max(s_t) - min(s_t)}\rVert^2}
    \label{eq:NRMSE}
\end{equation}

While these error metrics offer an estimate of the surrogate model's predictive capability for the next state, it is essential to assess its performance in predicting further into the future. This is crucial, as the model will be used for long-horizon control. To this end, the validation errors over a span of $T$ steps were computed. This was achieved by employing the learned surrogate model to make multi-step open-loop predictions. For each given sequence $(a_t, ..., a_{t+T})$ on the initial starting state $s_0$, a comparison was drawn between the corresponding ground-truth states $(s_t, ..., s_{t+T})$ and the multi-step state predictions $(\hat{s}_t, ..., \hat{s}_T)$ generated by the learned surrogate model. This comparison was formulated as:
\begin{equation}
    NRMSE^{T} = \sqrt{\frac{1}{D_{val}}\sum_{D_{val}}\frac{1}{T}\sum_{i=1}^{T}\lVert \frac{s_{t+i}-\hat{s}_{t+i}}{max(s_t) - min(s_t)}\rVert^2}, 
    \hat{s}_{t+i} = \begin{cases} 
                        s_t & i=0 \\
                        \hat{f}_\theta(s_t, a_t) & i>0
                    \end{cases}
    \label{eq:NRMSET}
\end{equation}
Furthermore, a further comparison was carried out, involving the ground-truth state $(s_{t+T})$ and the state prediction derived from the last computed step using the learned surrogate model $(\hat{s}_{t+T})$.  This comparison incorporated data from a total of $N$ observations and was assessed using the correlation coefficient ($R$), as defined by the following equation:
\begin{equation}
    \rho_{val}^{N} = \frac{\sum_{i=1}^{N}(s_i - \Bar{s}_i)(\hat{s}_i-\Bar{s}_{pred,i})}{\sqrt{\sum_{i=1}^{N}(s_i - \Bar{s}_i)^2 \sum_{i=1}^{N}(\hat{s}_i-\Bar{s}_{pred,i})^2}}
    \label{eq:R}
\end{equation}
where $\Bar{s}_{pred,i}$ represents the mean prediction of the surrogate model for state $s_i$ based on the actions $a_i$ and is calculated as: $\Bar{s}_{pred,i} = \frac{1}{n}\sum_{i=1}^{n}\hat{f}_\theta(s_i, a_i)$ 

Importantly, the T-step validation procedure was exclusively utilized for result analysis and was not employed during the actual training process.

\section{Neural Networks Design}
When considering the creation of a surrogate model to expedite the RL training process, as discussed in chapter \ref{chap2}, \ac{ANN} come to the forefront of the thoughts due to their inherent capacity to capture complex relationships within data and provide an efficient approximation of underlying dynamics. As highlighted, there exist multiple viable approaches within the realm of \ac{ANN}s, such as typical \ac{DNN}, \ac{RNN} and \ac{CNN}. Although \ac{CNN}  are a subset of feedforward neural networks, their convolutional layers are specifically tailored to discern spatial patterns and features within multidimensional and local receptive fields. This attribute renders them particularly suited for analyzing the influence of actions on subsequent states in a structured manner. However, given that the model at hand pertains to complex temporal relationships, the utilization of \ac{CNN} is not suitable within the scope of this thesis.

Belongs to \ac{ANN}, a typical \ac{DNN} consists of interconnected layers of artificial neurons, including the input layer, hidden layers, and output layer, which builds up the world of artificial intelligence and offers a straightforward design approach. The input layer receives raw data and transfers it to subsequent layers. The hidden layers undertake intermediate computations, employing activation functions to extract intricate features from the data. The output layer produces the final network output based on the assigned task, such as classification or regression. The design of the hidden layers can be meticulously tailored to accommodate various tasks, ensuring adaptability and versatility in tackling diverse computational challenges. 

Emerging as a distinct architecture within the realm of \ac{ANN}, \ac{RNN}s present a distinctive architecture. Setting them apart from the conventional feedforward neural networks, \ac{RNN}s incorporate loops within their structure, thereby enabling the assimilation of sequential and temporal information. This unique characteristic renders \ac{RNN}s especially adept at handling tasks characterized by sequences, time series data, and other dynamic patterns. Notably, each neuron within an \ac{RNN} not only processes the immediate input but also taps into information from preceding time steps, resulting in the creation of an internal memory mechanism. This inherent recurrence empowers \ac{RNN}s to effectively capture dependencies evolving over time and to manage sequences of varying lengths. Given these attributes, \ac{RNN}s are considered a choice for capturing the complex dynamics between actions and observations in time series data.

\subsection{Neural Network Architecture}
Continuing with the discussion of neural network architecture, it's important to underscore the significance of selecting an appropriate model for the specific task at hand. \ac{DNN} serve as a foundational architecture in the realm of artificial neural networks. They are characterized by an intricate interconnection of layers consisting of artificial neurons, including input, hidden, and output layers.

In the context of DNNs, raw data is initially received at the input layer and subsequently propagated through the network's layers. The intermediate computations occur within the hidden layers, which are strategically positioned between the input and output layers. These computations involve the application of activation functions at each pair of hidden layers. A commonly used activation function is \ac{ReLU}, known for introducing non-linearity by transforming negative values to zero while preserving positive values through element-wise activation. 

A fundamental component within DNNs is the fully connected layer, also referred to as the dense layer. This layer's distinguishing feature is that every neuron within it is interconnected with each neuron in the preceding and succeeding layers. This interconnectedness allows for comprehensive feature extraction and transformation. Neurons in the fully connected layer apply a linear transformation to the input vector using weight matrices. Subsequently, a non-linear activation function, denoted as $f$, is applied to the product: $y_{jk}(x)=f\left(\sum_{i=1}^{n_H} w_{j k} x_i+w_{j0}\right)$. Finally, the output layer generates the network's ultimate output, tailored to the specific regression task at hand. The structural arrangement of a DNN is visually depicted in Fig. \ref{fig:NN}.
\input{figures_hex/chap4/NN}

Deep Neural Networks are particularly adept at capturing intricate and complex relationships within data, making them well-suited for a wide range of applications, including image recognition, natural language processing, and more. In contrast, \ac{RNN}, characterized by loops in their structure, are ideal for handling sequential and temporal data, incorporating information from previous time steps to model temporal dynamics. Each neuron within an RNN not only processes the current input but also incorporates information from previous time steps. This inherent recurrence equips RNNs to capture and model temporal dynamics, which is essential for scenarios where actions influence subsequent states over time. RNNs employ loops to create a feedback mechanism where the output from one step becomes the input for the next. This cyclic connectivity enables RNNs to maintain a form of memory across different time steps. As an evolution of the RNN architecture, \ac{LSTM} layers incorporate specialized memory cells and gating mechanisms that enable them to selectively retain or forget information over extended sequences. These mechanisms regulate the flow of information through the network, allowing it to selectively remember or forget information over longer sequences.
\input{figures_hex/chap4/lstm}

In the context of the SoftQ surrogate model, a bidirectional LSTM architecture is also adopted, involving two distinct LSTM layers. The first layer processes the sequence in the traditional forward direction, while the second layer processes it in reverse. This bidirectional approach enhances the model's ability to capture bidirectional dependencies within the data. The architecture of the bidirectional LSTM RNN is visually depicted in Fig. \ref{fig:lstmstruct}. In addition to the LSTM layer, a dropout layer with a dropout rate of 50\% is incorporated into the architecture to enhance robustness and generalization capabilities. The dropout layer is a regularization technique that helps prevent overfitting by randomly setting a portion of the input neurons to zero during training.

With the architectural groundwork laid, 15\% of the training data was reserved for validation purposes, and the loss and \ac{RMSE} of the training is shown in Fig. \ref{fig:train2net}(a) and (c). Following this validation stage, the network was evaluated with some manual trot gait test data, the evaluation results are shown in Fig. \ref{fig:comp2net}.

\subsection{Optimization Algorithms}
Furthermore, it is essential to emphasize that alongside architectural considerations, the choice of optimization algorithms holds significant importance before the commencement of the training process. These algorithms steer the iterative parameter updates that guide the model towards convergence. Popular optimization algorithms include \ac{SGDM}, \ac{Adam}, and \ac{RMSProp}. These algorithms adjust the weights of the model based on the gradients of the loss function with respect to the parameters. They help to guide the model towards finding optimal parameter values that minimize the loss function.

\ac{SGDM} augments the standard Stochastic Gradient Descent (SGD) by introducing a momentum term. This term incorporates the weighted average of the previous gradients to determine the direction of the parameter updates. This addition imparts momentum to the optimization process, allowing for smoother convergence by reducing the oscillations that can occur during gradient updates. The momentum term helps to accelerate the movement along shallow directions while dampening oscillations in steep directions. 

\ac{Adam} is an optimization algorithm that combines the advantages of both momentum-based methods and adaptive learning rates. It maintains exponentially decaying moving averages of both past gradients and past squared gradients. These moving averages are used to compute adaptive learning rates for individual parameters. Additionally, \ac{Adam} employs bias correction mechanisms to counteract potential bias towards zero at the beginning of training. By adjusting the learning rates dynamically based on the historical gradients, \ac{Adam} enhances convergence speed and adaptability across varying gradients.

\ac{RMSProp} is an optimization technique designed to address the challenges of learning rate selection in \ac{SGDM}. It maintains an exponentially decaying average of squared gradients for each parameter. The learning rate is then divided by the square root of this average, which normalizes the learning rate by the historical gradient magnitudes. This adaptive learning rate adjustment helps to mitigate the problem of diminishing or exploding gradients.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{img/chap4/comp_op.eps}
    \caption{Comparison of Training Trajectories with Different Optimization Algorithms. The figure illustrates the training trajectories of two applied architectures while employing three different optimization algorithms: \ac{SGDM}, \ac{Adam}, \ac{RMSProp}. (a) Training loss and (b) validation loss for the bidirectional \ac{LSTM}-based \ac{RNN} architecture, (c) training loss (d) validation loss for the \ac{DNN} architecture.}
    \label{fig:comp3op}
\end{figure}

By implementing all three optimization algorithms in two networks, a comprehensive exploration was undertaken to discern the most suitable optimization strategy for the training of the surrogate model. This comprehensive approach facilitated a comprehensive assessment of each algorithm's impact on the training process and subsequent model performance. Specifically, the training was performed on both the bidirectional LSTM-RNN and DNN architectures, maintaining a consistent learning rate of 0.001. The training data was shuffled at the end of each epoch, and a mini-batch size of 512 was utilized. The training trajectories of the bidirectional LSTM-RNN and DNN networks, encompassing the utilization of the three optimization algorithms, are visually depicted in Figure \ref{fig:comp3op}. 

For the bidirectional LSTM-based RNN architecture, SGDM is the emerged as the optimal choice due to its remarkable ability to achieve the lowest training loss and converge rapidly. It exhibited both the fastest convergence and the least training loss among the evaluated optimization algorithms. In contrast, for the DNN architecture, took the lead. Adam demonstrated superior performance by learning rapidly and converging to the lowest loss values, both in validation and training. Its ability to efficiently navigate the DNN's training dynamics underscores its suitability for this architecture. Notably, all three optimization algorithms proved to be stable options for training this architecture effectively.

\subsection{Network Validation}
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{img/chap4/train_result.eps}
    \caption{Training of bidirectional LSTM-RNN architecture and DNN architecture. (a)-(b) Loss vs epoch for model training with bidirectional LSTM-based and DNN model; (c)-(d) RMSE vs epoch for model training with bidirectional LSTM-based and DNN model.}
    \label{fig:train2net}
\end{figure}

Nonetheless, when comparing the training performance of the bidirectional LSTM-based RNN architecture and the DNN architecture, as illustrated in Figure \ref{fig:train2net}, it becomes evident that the DNN architecture exhibits greater efficiency in terms of data utilization. Notably, the DNN showcases the capability to converge to a lower loss while achieving a reduced \ac{NRMSE}. This underscores the DNN's efficiency in leveraging the provided data and achieving enhanced predictive accuracy with a more compact loss function. Furthermore, evaluated against the real expert trot gait simulation data, the bidirectional LSTM-based RNN architecture achieves an NRMSE of approximately 0.61 for single step prediction, whereas the DNN attains a significantly lower NRMSE of around 0.26. This substantial divergence in NRMSE values highlights the DNN's proficiency in making more accurate predictions on these test cases. A more comprehensive review of the single-step predictions, as compared to the expert trot simulation data, can be observed in Figure \ref{fig:lstm_test} and Figure \ref{fig:DNN_test} in the Appendix. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{img/chap4/long2net.eps}
    \caption{Comparison of Long-Term Predictions using some expert trot gait data. (a) This figure illustrates the average correlation coefficient (R) of long-term predictions between the DNN architecture and the bidirectional LSTM-based RNN architecture. (b) This figure shows the \ac{NRMSE} of long-term predictions between the DNN architecture and the bidirectional LSTM-based RNN architecture.}
    \label{fig:comp2net}
\end{figure}

In addition, an evaluation of the long-term predictions produced by both architectures, as illustrated in Figure \ref{fig:comp2net}, reveals valuable insights. The average correlation coefficient (R) for the DNN architecture's single-step predictions exceeded 0.9, maintaining a commendable level of above 0.76 even for predictions spanning 10 seconds. While there was a slight increase in the \ac{NRMSE} for the DNN architecture from 0.26 to 0.35, the \ac{NRMSE} remained relatively stable in the case of the bidirectional LSTM-based RNN. Additionally, it was also observed from the correlation coefficient (R) of different prediction steps that the bidirectional LSTM-based RNN architecture did not exhibit substantial drops, demonstrating its stability in predicting long-term and time-series data with consistency. On the other hand, the bidirectional LSTM-based RNN architecture demonstrated a single-step correlation coefficient (R) of around 0.64 and an \ac{NRMSE} of approximately 0.62. For long-term predictions, these metrics do not surpass those achieved by the DNN architecture, indicating its limited ability to predict future states with high accuracy. Therefore, the DNN architecture optimized with the Adam optimization algorithm emerges as a promising choice for surrogate model training.

\subsection{Dataset Size}
The neural network's architecture, as discussed earlier, provided the framework for representing the complex relationships within the soft quadruped robot's behavior. However, the size of dataset remains a critical factor in enabling the neural network to learn effectively and achieve high estimation accuracy. The dataset's size is defined as a sequence of data, where each sequence data refers to the data collected during a sampling session that occurs without any failures. This definition is established to guarantee the consistency and reliability of the data employed in the neural network's training.

A sufficiently large dataset is essential as it provides the neural network with a diverse set of examples that encompass various scenarios and conditions. This diversity allows the network to generalize well and effectively capture the underlying relationships inherent in the dynamics of the soft quadruped robot. The consequence of having a small dataset is the risk of overfitting, where the network memorizes the training examples but does not truly comprehend the underlying patterns. Conversely, a larger dataset should aid the network in learning these underlying patterns more comprehensively and reduce the risk of overfitting. 

In Figure \ref{fig:datasize}, the average results of 5 long-prediction tests with different dataset size are shown. The correlation coefficient (R) remains similar at the single-step prediction level, but for long-term prediction, R increases significantly as the dataset size grows from 20 to around 220 sequences of data. Similarly, in terms of NRMSE, the error decreases from 20 sequences to around 280 sequences. These observations indicate that a larger dataset contributes to improved generalization, enhancing the network's ability to accurately estimate the behavior of the soft quadruped robot across a broader range of situations, including those has not encountered during sampling. This becomes particularly crucial when deploying the surrogate model in real-world scenarios where conditions may vary.

However, it's noteworthy that the trend reverses after 300 sequences of data and worsens further with 3000 sequences of data. The reason for the decline in performance with more data could be attributed to the fact that the random action data collected may not adequately characterize the useful information about the robot's dynamics. This could be due to the imbalanced distribution of data, with fewer samples representing high walking velocity, leading to the diminishing returns of adding more data in this form.
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{img/chap4/datasize.eps}
    \caption{Comparison of Different Data Size of Dataset $D$ in Long-Term Predictions using some expert trot gait data. (a) This figure illustrates the average correlation coefficient (R) of long-term predictions between the DNN architecture and the bidirectional LSTM-based RNN architecture. (b) This figure shows the \ac{NRMSE} of long-term predictions between the DNN architecture and the bidirectional LSTM-based RNN architecture.}
    \label{fig:datasize}
\end{figure}

It's crucial to strike a balance between dataset size and the associated computational resources and time required for training. Collecting and curating a substantial dataset demands significant effort and resources, and training a neural network on a large dataset can be computationally intensive. Therefore, careful consideration is required to determine an appropriate dataset size that aligns with the available resources while still achieving the desired level of accuracy and generalization. Based on these observations, selecting a dataset size approximately 20 times the dimensions of the observation data seems to strike a reasonable balance for the surrogate model in this thesis, which means about 200 sequences of data in this thesis.

\section{Model-based RL Algorithm}
After successfully training the surrogate model, the next phase of this thesis involves the application of \ac{MBRL} algorithms to further advance the research objectives. As previously discussed in Chapter \ref{chap2}, the \ac{SAC} algorithm has been selected due to its advantages in promoting efficient exploration and its compatibility with continuous action spaces. 

SAC addresses the challenge of dealing with continuous action spaces by incorporating an actor network that explicitly represents the policy function. This actor network is parameterized to output a probability distribution over actions, typically modeled as a Gaussian distribution with mean and standard deviation. SAC learns both the Q-function and the policy (actor) simultaneously through a combination of entropy-regularized policy optimization and Q-value estimation.

\subsection{SAC Algorithm}
The \ac{SAC} algorithm builds upon the principles of Actor-Critic but introduces a significant enhancement. To find an optimal gait controller through RL, the surrogate model is employed as an environment model within the framework of a Markov Decision Process ($S, A, P, r$). In this construct:
\begin{itemize}
    \item $S$ and $A$ characterize the continous state and action spaces, respectively.
    \item The transition function $P(s_{t+1} | s_t, a_t): S \times A \times S \rightarrow [0,+\infty)$ encapsulates the probability density governing an agent's transition from the current state $s_t \in S$ to a subsequent state $s_{t+1} \in S$ upon executing action $a_t \in A$. This function encapsulates the dynamics of the environment.
    \item Following the execution of action $a_t$ within state $s_t$, the agent is rewarded with an immediate feedback $r_t := r(s_t, a_t)$. The reward function $r: S \times A \to \mathbb{R}$ quantifies the desirability of an action within a given state.
\end{itemize}

The central objective of SAC is to derive a policy that maximizes the cumulative expected reward while also taking into account the entropy of the policy distribution. Specifically, if $X$ is a random variable and its probability density function is $p$, then its entropy $H(X)$ is defined as $H(X) = \mathbb{E}_{x\sim p}[-\ln p(x)]$, then the degree of stochasticity of a policy $\pi$ in a state $s_t$ could be represented as $H(\pi(\cdot|s_t)) = \mathbb{E}_\pi[-\ln \pi(a_t|s_t)]$. Policy entropy measures the randomness of the actions available to the agent, with higher entropy values indicating greater uncertainty. By including the entropy value in the objective function, exploration during training is encouraged, mitigating the risk of converging to a locally optimal policy. A non-deterministic control policy $\pi$ provides the conditional probability density $\pi(a_t|s_t)$ of taking action $a_t$ in a known state $s_t$. The goal is to find a stochastic policy that maximizes the sum of rewards and entropy. 

According to this Soft Bellman equation\cite{haarnojaSoftActorCriticOffPolicy2018}, the process of Soft Policy Iteration can ultimately converge to the Soft $Q$ function associated with policy $\pi$. Then in the policy improvement step, the policy is updated towards the exponential of the new soft $Q$ function. The iterative process of alternating between Soft policy evaluation and Soft policy improvement allows the final policy to converge to the optimal policy, as dictated by the objectives of maximum entropy reinforcement learning. In cases involving continuous spaces, it becomes necessary to approximate these iterations by parameterizing the $Q$ function and the policy $\pi$. In the maximum entropy RL, the problem can be defined to find the optimal policy $\pi^*$ each visited state, with the form of 
\begin{equation}
    \pi^* = \arg\max_\pi\mathbb{E}_\pi [\sum_t r(s_t,a_t)+\alpha H(\pi(\cdot|s_t))]
    \label{eq:bellman}
\end{equation} 
where $\alpha$ is the entropy regularization coefficient that explicitly controls the trade-off between exploration and exploitation. It determines the relative importance between the entropy term against the reward, and a higher $\alpha$ corresponds to higher exploration. 

In Soft Policy Iteration, the Soft Bellman equation plays a pivotal role in evaluating and improving the control policy. This equation calculates the expected reward for taking an action $a_t$ in a state $s_t$ and then following the policy $\pi$ with a discount factor $\gamma$.  This $Q$ function, which assesses the quality of a control policy, can be expressed as:
\begin{equation}
    Q_\pi(s_t,a_t) = r(s_t,a_t) + \gamma\mathbb{E}_{t+1}[V(s_{t+1})]
    \label{eq:Q}
\end{equation} 
Next, the value function $V_\pi(s_t)$ is defined to represent the expected cumulative reward when following policy $\pi$ from state $s_t$. It takes into account the Soft $Q$ function and incorporates an entropy term with a parameter $\alpha$:
\begin{equation}
    V_\pi(s_t) = \mathbb{E}_{a_t\sim \pi}[Q_\pi(s_t,a_t)] + H(\pi(\cdot|s_t)) = \mathbb{E}_{a_t\sim \pi}[Q_\pi(s_t,a_t) - \alpha\ln\pi(a_t|s_t)]
    \label{eq:valuef}
\end{equation}

For environments in continuous state and action spaces, the SAC algorithm employs Gaussian mixtures with mean and variance through the neural networks to model both the soft Q-function critic and the policy actor. Specifically, two action-value $Q$ functions are modeled, each parameterized by $\omega_1$ and $\omega_2$ and a policy function $\pi$, parameterized by $\phi$. SAC follows the concept of Double Deep Q-Network (DQN), using two networks and selecting the one with the smaller value to mitigate value overestimation issues. Both networks provide independent estimations of the value function through $Q_1(s_t, a_t)$ and $Q_2(s_t, a_t)$. Additionally, corresponding target networks, $Q_{\omega_1}(s_t, a_t)$ and $Q_{\omega_2}(s_t, a_t)$, are created to enhance optimization stability. During the update of both the $Q$ value function and the policy function, the minimum of the two target $Q$ functions is used for temporal difference target calculation. The loss function for the $Q$ functions, denoted as $L_Q(\omega)$, aims to minimize the soft Bellman residual: 
\begin{equation}
    \begin{aligned}
    &L_Q(\omega) = \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim R}\bigg[\frac{1}{2}\bigg(Q_\omega(s_t,a_t)-(r_t+\gamma V_\omega^- (s_{t+1}))\bigg)^2\bigg]\\
    &=\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim R,\;a_{t+1} \sim \pi_\phi(\cdot|s_{t+1})}\bigg[\frac{1}{2}\bigg(Q_\omega(s_t,a_t)-(r_t+\gamma(\min_{j=1,2}Q_{\omega_j}(s_t,a_t)-\alpha\ln\pi(a_{t+1}|s_{t+1}))\bigg)^2\bigg]
    \end{aligned}
    \label{eq:lossSAC}
\end{equation}
This loss function incorporates the temporal difference error, accounting for the difference between the predicted Q-value and the target value, which is a combination of the immediate reward and the estimated value of the next state. Additionally, the policy entropy term is included in the loss function. The loss function of the policy $\pi$ is obtained from the Kullback-Leibler divergence, which is simplified as: $$L_\pi(\phi)=\mathbb{E}_{s_t\sim R,a_t\sim \pi_\phi}[\alpha\ln(\pi_\phi(a_t|s_t))-Q_\omega(s_t,a_t)]$$The convergence and optimality of this approach have been rigorously established in previous works\cite{haarnojaSoftActorCriticAlgorithms2019, haarnojaSoftActorCriticOffPolicy2018}. 

To adaptively adjust the entropy regularization term, SAC reformulates the RL objective as an optimization problem with constraints. The aim is to maximize the expected return while ensuring that the mean entropy remains above a specified threshold $H_0$: $$\max_\pi\mathbb{E}_\pi \bigg[\sum_t (s_t,a_t)\bigg]\:s.t.\:\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}[-\ln (\pi_t(a_t|s_t))] \geq H_0\;\forall t$$ During the training process, the temperature parameter $\alpha$ is dynamically updated based on the entropy of the current policy, using past experiences. Initially set to 1, $\alpha$ gradually decreases to 0 as the training progresses. That is, maximising the expected return while constraining the mean value of entropy to be greater than $H_0$, the $\alpha$ will be updated by the loss function: $$L_\pi(\alpha) = \mathbb{E}_{s_t\sim R,a_t\sim\pi(\cdot|s_t)} [-\alpha\ln\pi(a_t|s_t)-\alpha H_0]$$ This adjustment reduces the emphasis on exploration when evaluating the value function. When the policy's entropy falls below the target value $H_0$, the training objective increases $\alpha$, giving more weight to the policy entropy term and encouraging exploration. Conversely, when the policy's entropy exceeds $H_0$, $\alpha$ decreases, focusing the policy training more on value enhancement.\input{figures_hex/chap4/SAC} 

In Figure \ref{fig:SAC}, an overview of this approach to learn a walking gait controller for a quadruped robot using the SAC algorithm is presented. The process begins with initialization, followed by the actor selecting an action $a_t$ at each training step based on the observation $s_t$, guided by the policy $\pi(\cdot|s_t)$. The selected action is executed, allowing the robot to interact with the environment and receive a reward $r_t$ along with the next observation $s_{t+1}$. These experiences, denoted as ($a_t, s_t, r_t, s_{t+1}$), are stored in the replay buffer $R$ and serve as training data for updating the neural network parameters. The agent periodically updates the parameters in the Q-value network, alternating between the two networks to mitigate value overestimation. Note that the actor–critic connection is represented in the update of the actor policy network, where its update utilizes the minimum of soft state value from target Q networks based on \ref{eq:valuef}. The policy actor network and temperature parameter are also updated through stochastic gradient descent, minimizing the residual across batch samples from the replay buffer $R$. The update process considers the minimum of soft state values from target Q networks, as described earlier. The learning rate $\lambda$ guides the parameter updates in the descent direction. This iterative process of interaction and learning continues until the gait controller converges to an optimal policy or reaches a predefined maximum number of episodes. For a more detailed mathematical exposition, please refer to the original works from Haarnoja et al.\cite{haarnojaSoftActorCriticAlgorithms2019, haarnojaSoftActorCriticOffPolicy2018}. This algorithm effectively addresses the challenges of continuous state and action spaces, providing a framework for maximum entropy reinforcement learning in such environments.

\subsection{Agent Specifications and Reward}
In the context of reinforcement learning, the definition of state, action, and reward constitutes a critical phase. This section provides an in-depth exposition of these fundamental aspects.
\subsubsection{State space}
The first consideration revolves around the definition of the state space, which is defined as the available sensor measurements from the robot. As discussed before, it includes the robot's orientation $\theta(t) =[\theta_x(t),\theta_y(t),\theta_z(t)]\:\in\:\mathbb{R}^3$ with regards to three dimensions, specifically roll, pitch, yaw; the moving velocities $v(t) =[v_x(t),v_y(t),v_z(t)]\:\in\:\mathbb{R}^3$ along three principal axes, $x$, $y$ and $z$; and the normalized contact force $f_n(t)=[f_{nFL}(t), f_{nFR}(t), f_{nRR}(t), f_{nRL}(t)]\:\in\:\mathbb{R}^4$ between each foot and ground. The above 10 dimensional measurements provide a comprehensive perspective on the robot's surroundings, enabling us to infer its internal state accurately. Additional insights into the nature and characteristics of these sensor measurements are available in Chapter \ref{chap3}. In addition, due to the computation and communication delays in real-world implementations, these temporal disparities introduce asynchrony between the observation of the environment and the execution of control actions. Notably, this asynchrony challenges the fundamental assumptions underpinning the Markov Decision Process, potentially resulting in substantial performance degradation, particularly in robot locomotion tasks. To address this challenge and ensure the effective modeling of the system's dynamics, a holistic approach is adopted. Specifically, the state representation is augmented by incorporating the action executed during the previous time step, denoted as $a_{t-1} \in \mathbb{R}^4$. This augmentation empowers the agent to consider the temporal dependencies of its actions and their influence on the current state. Consequently, the state vector, denoted as $s_t$, assumes an expanded form: $$s_t =[\theta(t),v(t),f_n(t),\mathbf{a}_{t-1}] \:\in\:\mathbb{R}^{14}$$ This enriched state representation equips the agent with an enhanced capacity to model the dynamics of its environment effectively. Furthermore, it enables the agent to grapple with the complexities arising from hardware latency in real-time robotic applications, ultimately advancing the pursuit of optimal gait control.
\subsubsection{Action space}
As previously elucidated, the locomotion of the robot is intricately governed by an inverse kinematics model, elegantly captured by Equation \ref{eq:value2motor}. In this context, the optimal gait is meticulously designed to propel the robot forward, with a particular emphasis on diagonal leg pairs. Consequently, the action space is framed around the articulation of these diagonal leg pairs. In precise terms, the action space is defined by two key components, desired bending angles $\alpha_b$ of each diagonal leg pair and compressed leg length $z_l$ of each diagonal leg pairs. For faster gradient descent, the actor output is normalized between 0 and 1 by the range $[-\mathbf{a}_{m}, \mathbf{a}_{m}]$ of the action spaces. Thus, the action space is defined as a continuous 4-dimensional space, and a single action $\mathbf{a}_t$ is a vector containing 4 desired motor positions after normalization, i.e., $\mathbf{a}_t \:\in\:[0,1]^4$ Subsequently, the policy network output is converted by the following action to reference leg locomotion mapping $\alpha_{bref} = 2\alpha_{bmax}(a_t -0.5)$ for bending angle and $z_{lref} = z_{lmax}a_t$.
\subsubsection{Reward}
The primary objective in this study is to encourage the robot maintain a straight and stable gait over extended periods. Hence, the objective reward function is formulated and defined as: 
\begin{equation}
    r(s_t,a_t) = \epsilon_1\frac{T_s}{T_f} + (1-\epsilon_2|v_x(t)-v_{ref}|)-\epsilon_3\lVert\Ddot{\mathbf{a}_t}\rVert - \epsilon_4\lVert\mathbf{a}_t-\mathbf{\sigma}_{threshold}\rVert
    \label{eq:reward}
\end{equation}
In each episode, the control steps continue until the robot falls or the simulation fails. The criteria of failing are identical to that of simulation termination as previously defined in Section \ref{sec3.2}. At each training step, the robot is rewarded with a constant value $\epsilon_1\frac{T_s}{T_f}$ for maintaining balance without triggering any terminal conditions. Here $T_s$ denotes the sampling time of the agent's interaction with the simulation environment, and $T_f$ is the final time of this training in one episode. Instead of applying significant penalties upon episode termination due to undesired states, continuous rewards at each step for the robot's longevity in the task are provided. The accumulated rewards for this term are proportional to the sequence length, effectively motivating the agent to remain upright for extended duration. 

As a walking gait controller, the agent is trained to encourage the robot to consistently move in close alignment with a reference speed $v_{ref}$ along the $x$-axis. To achieve this, a reward component that regulates the forward velocity $v_x(t)$. This component utilizes a shifted absolute value function, with $\epsilon_2$ governing the shape of the reward function. The essential idea  is to stimulate the robot to maintain forward motion with a positive speed while discouraging backward movement (negative speed). Therefore, the choice of $\epsilon_2$ is dependent on the reference speed $v_{ref}$, with a larger magnitude of $\epsilon_2$ favored when $v_{ref}$ decreases. This sharpens the reward decay near $v_{ref}$ and penalizes negative velocities. For simplicity, we set $\epsilon_2 = −1/v_{ref}$ to ensure that the maximum reward is unity, and negative speeds are penalized. 

To promote stable action on motors, a penalty on the large action acceleration $\Ddot{\mathbf{a}_t}$ quantified by $\epsilon_3$ is introduced. This penalty discourages abrupt changes in motor actions, aiming to deliver a stable actuation signal to the motors. The penalty acceleration value is estimated using finite differences of actions from the last three time steps: $$\Ddot{\mathbf{a}_t} = \frac{\mathbf{a}_t+\mathbf{a}_{t-2}-2\mathbf{a}_{t-1}}{T_s^2}$$ Note that $\mathbf{a}_{t-1}$ and $\mathbf{a}_{t-2}$ are not included in the state space at the time step $t$, but is recorded in the robot's memory.

In addition, to ensure stable walking gait control, the excessive bending angles $\alpha_b$ of the robot's leg pairs are penalized, where negative rewards are assigned as $- \epsilon_4\lVert\mathbf{a}_t-\sigma_{threshold}\rVert$, serving as a discouragement for excessive leg bending. $\sigma_{threshold}$ is the threshold for bending angle to prevent simultaneous bending of all four legs. The weighting factor $\epsilon = [\epsilon_1, \epsilon_2, \epsilon_3, \epsilon_4]$ objectively defines the emphasis placed on each aspect of the reward function. These values are set to $[10, −1/v_{ref}, 10, 100]$, aligning with the objective of training the robot effectively.

\subsection{Training Setup}
As previously mentioned, two Q-value critic networks are employed, each complemented by its corresponding target critic network. Their primary purpose is to estimate the $Q$ function as defined in Eq. \ref{eq:Q}. These critic networks share a uniform architecture, characterized by two distinct input components: observations and actions. The observation section undergoes processing through two fully-connected layers, each comprising 128 units. Meanwhile, the action component is managed by a single fully-connected layer with 128 units. The outputs from both sections are concatenated and subsequently channeled through another fully-connected layer featuring 32 units, with the activation function being Rectified Linear Unit (ReLU). The ultimate output of the Q network serves as an estimation of the $Q$ function. 

Concurrently, the actor network utilizes the environment observation as its input. This input is processed through two fully-connected layers, each containing 256 units, and employs ReLU as the activation function. The core responsibility of the actor network is to predict the mean and variance for each action through Gaussian distributions. To ensure that the generated actions fall within the [0, 1] range, the hyperbolic tangent function (tanh) is employed as a squashing function. 

To mitigate the risk of overfitting, the learning rates for both the critic and actor networks are set at 0.002 and 0.001, respectively. The learning rate for the temperature parameter $\alpha$ is kept consistent with the network learning rates, fixed at 0.001. For the sake of stability during updates, the update frequency of the policy network is reduced, with parameters being updated every 3 steps in the simulation. The discount factor $\gamma$ is firmly set at 0.96, as the number of complete steps in a training episode is 50. Furthermore, the target entropy is defined based on the number of actions, with a specific value of $H'=-4$. The batch size is set to 4096($2^{12}$), and the size of the replay buffer is 16384($2^{14}$). To ensure uniformity in environment initialization, all episodes begin from the same initial state $s_0$.

To increase the robustness of trained agent, zero mean Gaussian noise is introduced into all observation signals. This additional noise is independently sampled from zero-mean Gaussian distributions in a random manner. Based on the sensor calibration results, the variances of the noise applied to the velocity, angular and force signal data are set to: $\sigma_\theta^2 = 0.002$, $\sigma_{\mathbf{v}}^2 = 0.002$ and $\sigma_{\mathbf{f}_n}^2 = 0.005$, respectively.

\section{Control Architecture Design}
The control architecture design of the soft quadruped robot is a critical component in achieving stable and efficient locomotion. In essence, the control architecture of the soft quadruped robot represents a harmonious synthesis of hardware, software, and sensor integration. This design enables stable and efficient locomotion, with servo motors, the STM32 microcontroller, and seamless communication underpinning precise control. Additionally, sensory observations gathered through a diverse array of sensors empower the robot with heightened environmental perception and adaptability. The Simulink controller, driven by observations and control commands, ensures the robot's actions align with its intended behaviors, ultimately culminating in stable and efficient locomotion. The control architecture of SoftQ is visually depicted in Figure \ref{fig:control}.
\input{figures_hex/chap4/control}

The locomotion of SoftQ is profoundly governed by servo motors, which receive \ac{PWM} signals from the STM32 microcontroller. These signals translate into specific rotational torques applied to the actuators, leading to controlled limb motions for different gaits. The servo motors feature built-in position controllers that respond to PWM signals, ensuring precise motor positioning.

The STM32 microcontroller manages contact forces and controls the servo motors through \ac{GPIO} pins. It efficiently processes commands from the Raspberry Pi, generating PWM signals in response to Simulink controller commands. This fine-grained control guarantees accurate execution of the robot's intended actions. Additionally, the STM32 interfaces with force sensors, reading voltage values across them. These voltage changes help the controller determine the applied forces' magnitude, serving as feedback for the robot to adapt its movements to external forces. Communication between the Raspberry Pi and STM32 utilizes \ac{UART} for command and data exchange.

The Raspberry Pi directly interfaces with a variety of sensors and devices to gather supplementary information for guiding the robot's control. These vital observations encompass quaternion angles, linear accelerations, and distance data, all of which are acquired through the \ac{I²C} interface. The quaternion angles, originating from the \ac{IMU}, provide a representation of the robot's orientation in three-dimensional space. To enhance the situational awareness, these quaternion angles are converted into Euler angles, specifically roll, pitch, and yaw. The IMU itself is a sophisticated 9-DOF sensor, featuring integrated sensor fusion capabilities via its onboard microcontroller. This functionality allows the IMU to provide additional fused data, including linear accelerations in three axis. The linear accelerations is subsequently integrated with distance data from \ac{ToF} sensors to estimate the robot's actual velocity. ToF sensors employ the time-of-flight principle by emitting light pulses that bounce off objects, returning to the sensor to calculate the mean distance within a square area in front of it, measuring the distance between the sensor and the reflecting object.

In the context described, the Simulink controller plays a crucial role by assuming the task of generating the robot's actions through external mode operation on the Raspberry Pi. This entails the generation of control commands, which are determined based on the assessment of states inferred from direct observations. To facilitate this process, raw data such as orientation, linear accelerations, and contact forces of the robot are subjected to preprocessing, transforming them into states ($\mathbf{s}_t$) that can serve as inputs for the actor network. The actor network, which is derived from a converged SAC (Soft Actor-Critic) algorithm, serves as the core component in this control scheme. The actor network takes on the critical task of planning an optimal gait, generating actions that enable the robot to move forward effectively. However, it is essential to note that these actions, produced by the actor network, must undergo a further transformation before they can be effectively utilized by the lower-level controllers. This transformation step involves the conversion of the actions into motor commands. This conversion process relies on inverse kinematic relations, as outlined in Equation \ref{eq:value2motor}. This ensures that the actions generated by the actor network are translated into commands that directly influence the robot's motors, thereby enabling the desired movements and control of the robot's behavior.

Furthermore, the Raspberry Pi establishes wireless communication with a PC to enhance its functionality. The PC serves as a conduit for transferring MATLAB Simulink programs to the Raspberry Pi via external mode. It also acts as a Graphical User Interface (GUI), enabling seamless human-robot interaction. The GUI offers real-time status updates, providing immediate insights into the robot's performance and operation. Moreover, it facilitates the reception of tasks and instructions from the operator, allowing real-time directive issuance. This bidirectional communication between the PC and the robot enhances versatility and adaptability. Additionally, the control system accommodates user input, offering various interfaces like joysticks or command line velocity inputs for real-time robot guidance and adjustments.

\subsection{Task Planning and Execution}
The control architecture of the soft quadruped robot encompasses a multifaceted orchestration of task planning and execution, which plays a pivotal role in the robot's ability to achieve its goals with precision and adaptability. This section delves into the specific architecture of task planning and execution at software side within the control framework. 

The core of the robot's control architecture is centered around the efficient utilization of multi-threading. Each thread is responsible for a specific aspect of the robot's operation, and these threads run concurrently, allowing for parallel processing and efficient resource management. This concurrent execution is particularly advantageous in enhancing the robot's ability to perform multiple tasks simultaneously. Notably, the Raspberry Pi serves as the primary processing unit responsible for coordinating the robot's actions. Within this Raspberry Pi-based control unit, four distinct threads operate in unison to orchestrate the robot's functions: 
\begin{itemize}
    \item UDP\_Transmit Thread (20 Hz): This thread is responsible for reading, composing and transmitting observational data to the Simulink environment via User Datagram Protocol (UDP). It facilitates the seamless exchange of sensory information gathered by the robot with the Simulink simulation. The frequency is bounded by the I²C communication of 5 ToFs. 
    \item UDP\_Receive Thread (50 Hz): The UDP\_Receive thread specializes in receiving motor control commands from the Simulink environment via UDP communication. It ensures that the robot can receive real-time instructions from the simulation, allowing for dynamic control adjustments. Importantly, this thread operates conditionally, only receiving commands when a specific counter condition is met, contributing to safety and controlled execution.
    \item com2STM32 Thread (20 Hz): This thread is responsible for bidirectional communication with the STM32 microcontroller, which interfaces with the robot's hardware components. It sends relevant control data to the STM32 unit, enabling the robot to perform physical actions. Simultaneously, it receives sensor readings and feedback from the STM32, providing crucial information about the robot's state and surroundings. The frequency is bounded by the computation load from Simulink. 
    \item Countdown Thread (0.5 Hz): The Internal Countdown thread operates at a lower frequency of 0.5 Hz. Its primary role is to manage a countdown mechanism that regulates the activation of flags to the STM32, indicating when motor commands received from Simulink can be executed.  
\end{itemize}

One fundamental aspect of this architecture is its decentralized nature. Instead of relying on a single central controller, the robot employs a distributed control strategy. Each thread is designed to handle a specific subset of tasks or functionalities. For instance, the STM32 process operates on an event-triggered basis, only executing motor commands when received from the Raspberry Pi. It also sends back contact force readings, adding a feedback loop to the control process. Additionally, the presence of flags controlling the switch of power sources ensures that only reliable motor commands obtained from Simulink are executed, further enhancing the safety and reliability of the robot's actions. 
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{img/chap4/process.pdf}
    \caption{Schematics of multi-core communication of SoftQ.}
    \label{fig:process}
\end{figure}

To facilitate cooperation among these threads and processes, a well-defined mechanism for multi-core communication and data sharing is established. This mechanism allows threads to exchange information and synchronize their actions. The schematic representation of this multi-core communication is illustrated in Figure \ref{fig:process}. To exemplify this inter-thread communication, consider the SoftQ process running on the Raspberry Pi. Within this context, a "Thread-Safe Data Access" mechanism is meticulously implemented. This involves encapsulating shared data within a dataclass, a structured approach that helps prevent data corruption or conflicts in a multi-threaded environment. Furthermore, synchronization mechanisms such as locks or semaphores are judiciously employed. These synchronization tools serve as safeguards, ensuring that only one thread can access and modify shared data at any given time, thus mitigating the risk of data inconsistency or errors. In addition, the control architecture benefits from pre-designed DMA (Direct Memory Access) and Interrupts Muralidharan et al.\cite{thorapallimuralidharanContinuumActuatorBased2020} and Danelia et al.\cite{daneliaStructureGaitOptimizationof2021}. These advanced techniques enhance data transfer efficiency and response times, further augmenting the control system's capabilities.

\subsection{Sensor Fusion by Kalman Filter}
However, as a core objective in this thesis, the velocity should be as more accurate and precise as possible. To address this objective, a Kalman filter has been specifically designed and implemented within the control architecture. This Kalman filter serves the crucial role of reducing sensor electronic noise, measurement inaccuracies, sensor noise, and quantization errors when predicting velocity from acceleration and position measurements.

The Kalman filter is initiated with carefully chosen parameters that define its behavior. states $x=[p,v]$ 
\begin{equation}
    \begin{aligned}
        \text{State Equation:} \quad & \mathbf{x}_{k} = \mathbf{A}_{k} \mathbf{x}_{k-1} + \mathbf{B}_{k} \mathbf{u}_{k} + \mathbf{w}_{k} \\
        \text{Measurement Equation:} \quad & \mathbf{z}_{k} = \mathbf{H}_{k} \mathbf{x}_{k} + \mathbf{v}_{k}
    \end{aligned}
\label{eq:kf}
\end{equation}
$\mathbf{x}_{k}$ represents the state vector at time step $k$.
$\mathbf{A}_{k}$ is the state transition matrix at time step $k$, which describes how the state evolves from the previous time step.
$\mathbf{B}_{k}$ is the control-input matrix at time step $k$, which represents any control inputs or external forces acting on the system.
$\mathbf{u}_{k}$ represents the control input vector at time step $k$.
$\mathbf{w}_{k}$ is the process noise vector at time step $k$, representing the uncertainty or noise in the state transition process.
$\mathbf{z}_{k}$ represents the measurement vector at time step $k$.
$\mathbf{H}_{k}$ is the measurement matrix at time step $k$, which relates the state vector to the measurements.
$\mathbf{v}_{k}$ is the measurement noise vector at time step $k$, representing the uncertainty or noise in the measurements.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{img/chap4/KF.eps}
    \caption{Velocity Estimation in x-axis ($v_x$) through Kalman Filtering of ToF and IMU data.}
    \label{fig:KF}
\end{figure}

Even though Euler angles are used to represent rotations, the transmission of two poses are calculated by Quaternion, which is a mathematical construct that extends the notion of complex numbers to four dimensions. They comprise four components: a scalar part represented by $w$, and a vector component denoted as $\vec{v}=(x, y, z)$. Quaternions are expressed as:
$$q = w + xi + yj + zk$$
where $x$, $y$, $z$, and $w$ are real numbers, and $i$, $j$, and $k$ are three imaginary units that satisfy the following multiplication rules: $i^2 = j^2 = k^2 = ijk = -1$. This approach involves establishing the initial pose of the robot as a quaternion and subsequently obtaining the current pose through sensors like accelerometers and gyroscopes. By computing the change in pose between the initial and current states, the rotational component can be extracted. This rotational component provides insights into the rotation axis and angle. To determine the robot's velocity, differentiation of the position component in the current pose is performed with respect to time. The resultant velocity vector is then transformed into the world coordinate frame, leveraging the robot's current orientation. This computation culminates in the derivation of the angular velocity vector and the linear velocity vector, which serve as the final outcomes of this process. Eventually, with all subsystems modeled, the quadruped robot system is assembled and simulated in the MATLAB Simscape environment.

\subsection{Potential Reality Gap}
Transitioning to a different facet of consideration, it's crucial to address the concept of the "Potential Reality Gap." In the realm of reinforcement learning, the surrogate model serves as a bridge between simulation environments and real-world scenarios. However, due to the inherent complexity and uncertainty associated with real-world environments, a misalignment often arises between the training data collected from simulations and the actual performance of the trained model in real-world situations. This discrepancy is referred to as the "Reality Gap."

The Reality Gap poses a challenge because the surrogate model might perform exceptionally well during training in simulation environments but struggle to generalize effectively to real-world settings. This phenomenon can stem from several factors, including variations in physical dynamics, sensor noise, and unmodeled environmental influences. To address the Reality Gap, certain strategies can be adopted. One approach involves introducing stochasticity during training in simulations, simulating uncertainties and disturbances that are characteristic of real-world scenarios. This helps the model develop robustness to unexpected variations.